{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 28, 28) (59950, 28, 28) (50,) (59950,) (10000, 28, 28) (10000,)\n",
      "(60000, 784) (50, 784) (10000, 784) (59950, 784)\n"
     ]
    }
   ],
   "source": [
    "(train_pool_X, train_pool_y), (test_X, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "train_X, Pool_X, train_y,  Pool_y = train_test_split(train_pool_X, train_pool_y, train_size=5, random_state=42)\n",
    "print(train_X.shape, Pool_X.shape, train_y.shape, Pool_y.shape, test_X.shape, test_y.shape)\n",
    "train_pool_X = train_pool_X.reshape(train_pool_X.shape[0], -1)\n",
    "train_X = train_X.reshape(train_X.shape[0], -1)\n",
    "test_X = test_X.reshape(test_X.shape[0], -1)\n",
    "Pool_X = Pool_X.reshape(Pool_X.shape[0], -1)\n",
    "print(train_pool_X.shape, train_X.shape, test_X.shape, Pool_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN,self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28,128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128,10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,28*28)\n",
    "        x= F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming your dataset is in NumPy arrays\n",
    "# train_X, train_y, test_X, test_y, Pool_X, Pool_y = ...\n",
    "\n",
    "# Reshape the data\n",
    "train_X = train_X.reshape(train_X.shape[0], -1)\n",
    "test_X = test_X.reshape(test_X.shape[0], -1)\n",
    "Pool_X = Pool_X.reshape(Pool_X.shape[0], -1)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "train_X_tensor = torch.tensor(train_X, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(train_y, dtype=torch.long)\n",
    "test_X_tensor = torch.tensor(test_X, dtype=torch.float32)\n",
    "test_y_tensor = torch.tensor(test_y, dtype=torch.long)\n",
    "Pool_X_tensor = torch.tensor(Pool_X, dtype=torch.float32)\n",
    "\n",
    "# Define batch size for DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoader for the unlabeled dataset (Pool_X_tensor)\n",
    "unlabeled_dataset = TensorDataset(Pool_X_tensor)\n",
    "unlabeled_data_loader = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define your SimpleNN model class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, enable_dropout=False):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if enable_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def enable_dropout(self):\n",
    "        \"\"\"\n",
    "        Enable dropout layers during model evaluation.\n",
    "        \"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.train()\n",
    "\n",
    "# Instantiate the SimpleNN model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_data_loader):\n",
    "        # Reshape images if necessary\n",
    "        images = images.view(-1, 28*28)\n",
    "\n",
    "        # Move images and labels to the appropriate device (e.g., GPU)\n",
    "        images = images.to(torch.device('cuda'))\n",
    "        labels = labels.to(torch.device('cuda'))\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training progress\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_data_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Active Learning with Epistemic Uncertainty using Monte Carlo Dropout\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_monte_carlo_predictions(data_loader, forward_passes, model, n_classes, n_samples):\n",
    "\n",
    "    dropout_predictions = np.empty((0, n_samples, n_classes))\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    for i in range(forward_passes):\n",
    "        predictions = np.empty((0, n_classes))\n",
    "        model.eval()\n",
    "        enable_dropout(model)\n",
    "        for i, image in enumerate(data_loader):\n",
    "            image = image.to(torch.device('cuda'))\n",
    "            with torch.no_grad():\n",
    "                output = model(image)\n",
    "                output = softmax(output)  \n",
    "            predictions = np.vstack((predictions, output.cpu().numpy()))\n",
    "\n",
    "        dropout_predictions = np.vstack((dropout_predictions, predictions[np.newaxis, :, :]))\n",
    "\n",
    "    mean = np.mean(dropout_predictions, axis=0)\n",
    "\n",
    "    return mean\n",
    "\n",
    "def enable_dropout(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.train()\n",
    "\n",
    "def entropy(p):\n",
    "    return -np.sum(p * np.log2(p + 1e-8), axis=1)  # Adding a small value to avoid log(0) errors\n",
    "\n",
    "def active_learning_with_uncertainty(data_loader, forward_passes, model, n_classes, n_samples, alpha=0.5, num_samples_to_label=100):\n",
    "    dropout_predictions = get_monte_carlo_predictions(data_loader, forward_passes, model, n_classes, n_samples)\n",
    "\n",
    "    # Calculate epistemic uncertainty using variance of predictions\n",
    "    variance = np.var(dropout_predictions, axis=0)\n",
    "\n",
    "    # Calculate entropy of the predicted probabilities\n",
    "    mean = np.mean(dropout_predictions, axis=0)\n",
    "    entropies = entropy(mean)\n",
    "\n",
    "    # Calculate the uncertainty score (a weighted combination of variance and entropy)\n",
    "    uncertainty_score = alpha * variance + (1 - alpha) * entropies\n",
    "\n",
    "    # Sort the samples based on the calculated uncertainty scores\n",
    "    sorted_indices = np.argsort(uncertainty_score)\n",
    "\n",
    "    # Select the most uncertain samples for labeling\n",
    "    selected_indices = sorted_indices[-num_samples_to_label:]\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "# Active Learning Loop\n",
    "num_iterations = 5\n",
    "num_samples_to_label = 100\n",
    "forward_passes = 10  # You can adjust the number of forward passes as per your requirements\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    print(f\"Active Learning Iteration {iteration + 1}/{num_iterations}\")\n",
    "    \n",
    "    # Step 1: Select the most uncertain samples from the unlabeled dataset\n",
    "    selected_indices = active_learning_with_uncertainty(\n",
    "        data_loader=unlabeled_data_loader,\n",
    "        forward_passes=forward_passes,\n",
    "        model=model,\n",
    "        n_classes=10,  # Replace with the actual number of classes in your dataset\n",
    "        n_samples=num_samples_to_label\n",
    "    )\n",
    "\n",
    "    # Step 2: Label the selected samples and add them to the training dataset\n",
    "    labeled_images = Pool_X_tensor[selected_indices]\n",
    "    labeled_labels = Pool_y_tensor[selected_indices]\n",
    "\n",
    "    # Combine the labeled samples with the original training dataset\n",
    "    train_X_tensor = torch.cat([train_X_tensor, labeled_images])\n",
    "    train_y_tensor = torch.cat([train_y_tensor, labeled_labels])\n",
    "\n",
    "    # Create a new DataLoader for the updated training dataset\n",
    "    train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Step 3: Retrain the model on the updated training dataset\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, (images, labels) in enumerate(train_data_loader):\n",
    "            # Reshape images if necessary\n",
    "            images = images.view(-1, 28*28)\n",
    "\n",
    "            # Move images and labels to the appropriate device (e.g., GPU)\n",
    "            images = images.to(torch.device('cuda'))\n",
    "            labels = labels.to(torch.device('cuda'))\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print training progress\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_data_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# After the active learning iterations, your model should be trained on the updated dataset.\n",
    "# You can now evaluate the model on the test dataset to measure its performance.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_X_tensor = test_X_tensor.to(torch.device('cuda'))\n",
    "    test_y_tensor = test_y_tensor.to(torch.device('cuda'))\n",
    "    outputs = model(test_X_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == test_y_tensor).sum().item() / len(test_y_tensor)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_dropout(model):\n",
    "    for m in model.modules():\n",
    "        if m.__class__.__name__.startswith('Dropout'):\n",
    "            m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monte_carlo_predictions(data_loader,\n",
    "                                forward_passes,\n",
    "                                model,\n",
    "                                n_classes,\n",
    "                                n_samples):\n",
    "\n",
    "    dropout_predictions = np.empty((0, n_samples, n_classes))\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    for i in range(forward_passes):\n",
    "        predictions = np.empty((0, n_classes))\n",
    "        model.eval()\n",
    "        enable_dropout(model)\n",
    "        for i, (image, label) in enumerate(data_loader):\n",
    "            image = image.to(torch.device('cuda'))\n",
    "            with torch.no_grad():\n",
    "                output = model(image)\n",
    "                output = softmax(output)  \n",
    "            predictions = np.vstack((predictions, output.cpu().numpy()))\n",
    "\n",
    "        dropout_predictions = np.vstack((dropout_predictions,\n",
    "                                         predictions[np.newaxis, :, :]))\n",
    "\n",
    "    mean = np.mean(dropout_predictions, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    return -np.sum(p * np.log2(p), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predictions(model, X, num_samples=10):\n",
    "    model.train()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            output = model(X)\n",
    "            predictions.append(output.softmax(dim=1).numpy())\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, test_X, test_y):\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        test_X_tensor = torch.Tensor(test_X).to(torch.float32)\n",
    "        outputs = model(test_X_tensor)\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        correct = (predicted_labels == torch.Tensor(test_y)).sum().item()\n",
    "        total = len(test_y)\n",
    "        accuracy = correct / total\n",
    "        # print('Accuracy: %.2f' % (accuracy*100))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning(train_X,train_y,pool_X_train,pool_y_train,pool_X_test,y_test,acquisition_function,num_iterations,number_of_sample=10):\n",
    "    model=SimpleNN()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    new_indices_list=[]\n",
    "    train_accuracies_al=[]\n",
    "    test_accuracies_al=[]\n",
    "    for i in range(num_iterations):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(torch.Tensor(pool_X_train).to(torch.float32))\n",
    "        probablities = F.softmax(output, dim=1).detach().numpy()\n",
    "        acquisition_score=acquisition_function(probablities)\n",
    "        new_indices = np.argsort(acquisition_score)[::-1][:number_of_sample]\n",
    "        new_indices_list.append(new_indices)\n",
    "        labeled_x=pool_X_train[new_indices]\n",
    "        labeled_y=pool_y_train[new_indices]\n",
    "        train_X=np.concatenate((train_X,labeled_x),axis=0)\n",
    "        train_y=np.concatenate((train_y,labeled_y),axis=0)\n",
    "        pool_X_train=np.delete(pool_X_train,new_indices,axis=0)\n",
    "        pool_y_train=np.delete(pool_y_train,new_indices)\n",
    "        train_X_tensor=torch.Tensor(train_X).to(torch.float32)\n",
    "        train_y_tensor=torch.Tensor(train_y).to(torch.long)\n",
    "        output = model(train_X_tensor)\n",
    "        loss = criterion(output, train_y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return train_accuracies_al,test_accuracies_al,new_indices_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning_mc_dropout(train_X, train_y, pool_X_train, pool_y_train, pool_X_test, y_test, num_iterations, num_samples=10):\n",
    "    model=SimpleNN()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []   \n",
    "    new_indices = []\n",
    "    for i in range(num_iterations):\n",
    "        model.train()\n",
    "        optimizer.zero_grad() \n",
    "        output = mc_dropout_predictions(model,torch.Tensor(pool_X_train), num_samples)\n",
    "        uncertainty = np.var(output, axis=0).sum(axis=1)\n",
    "        new_index = np.argsort(uncertainty)[-1:]\n",
    "        new_indices.append(new_index)\n",
    "        label_X = pool_X_train[new_index]\n",
    "        label_y = pool_y_train[new_index]\n",
    "        train_X = np.concatenate((train_X, label_X), axis=0)\n",
    "        train_y = np.concatenate((train_y, label_y), axis=0)\n",
    "        pool_X_train = np.delete(pool_X_train, new_index, axis=0)\n",
    "        pool_y_train = np.delete(pool_y_train, new_index)\n",
    "        train_X_tensor = torch.Tensor(train_X).to(torch.float32)\n",
    "        train_y_tensor = torch.Tensor(train_y).to(torch.long)\n",
    "        output = model(train_X_tensor)\n",
    "        loss = criterion(output, train_y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_accuracy = calculate_accuracy(model, train_X_tensor, train_y_tensor)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        model.eval()\n",
    "        test_accuracy = calculate_accuracy(model, torch.Tensor(pool_X_test).to(torch.float32), y_test)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "    return train_accuracies, test_accuracies, new_indices\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2059/3723900648.py:2: RuntimeWarning: divide by zero encountered in log2\n",
      "  return -np.sum(p * np.log2(p), axis=1)\n",
      "/tmp/ipykernel_2059/3723900648.py:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -np.sum(p * np.log2(p), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.67\n",
      "Accuracy: 25.99\n",
      "Accuracy: 54.29\n",
      "Accuracy: 33.27\n",
      "Accuracy: 68.75\n",
      "Accuracy: 40.87\n",
      "Accuracy: 78.89\n",
      "Accuracy: 49.95\n",
      "Accuracy: 84.00\n",
      "Accuracy: 57.89\n",
      "Accuracy: 89.09\n",
      "Accuracy: 61.33\n",
      "Accuracy: 90.83\n",
      "Accuracy: 63.02\n",
      "Accuracy: 90.00\n",
      "Accuracy: 63.31\n",
      "Accuracy: 92.86\n",
      "Accuracy: 63.31\n",
      "Accuracy: 94.67\n",
      "Accuracy: 64.10\n",
      "Accuracy: 94.38\n",
      "Accuracy: 64.74\n",
      "Accuracy: 96.47\n",
      "Accuracy: 65.39\n",
      "Accuracy: 97.78\n",
      "Accuracy: 65.60\n",
      "Accuracy: 96.84\n",
      "Accuracy: 65.88\n",
      "Accuracy: 97.00\n",
      "Accuracy: 66.40\n",
      "Accuracy: 96.67\n",
      "Accuracy: 67.00\n",
      "Accuracy: 96.82\n",
      "Accuracy: 67.31\n",
      "Accuracy: 97.39\n",
      "Accuracy: 67.59\n",
      "Accuracy: 97.92\n",
      "Accuracy: 67.95\n",
      "Accuracy: 98.00\n",
      "Accuracy: 68.10\n",
      "Accuracy: 98.85\n",
      "Accuracy: 68.33\n",
      "Accuracy: 98.89\n",
      "Accuracy: 68.42\n",
      "Accuracy: 99.64\n",
      "Accuracy: 68.45\n",
      "Accuracy: 99.66\n",
      "Accuracy: 68.71\n",
      "Accuracy: 100.00\n",
      "Accuracy: 68.69\n",
      "Accuracy: 100.00\n",
      "Accuracy: 68.88\n",
      "Accuracy: 99.69\n",
      "Accuracy: 68.77\n",
      "Accuracy: 99.70\n",
      "Accuracy: 69.11\n",
      "Accuracy: 99.71\n",
      "Accuracy: 69.22\n",
      "Accuracy: 99.71\n",
      "Accuracy: 69.41\n",
      "Accuracy: 99.72\n",
      "Accuracy: 69.51\n",
      "Accuracy: 100.00\n",
      "Accuracy: 69.57\n",
      "Accuracy: 100.00\n",
      "Accuracy: 69.65\n",
      "Accuracy: 100.00\n",
      "Accuracy: 69.59\n",
      "Accuracy: 100.00\n",
      "Accuracy: 69.70\n",
      "Accuracy: 99.76\n",
      "Accuracy: 69.63\n",
      "Accuracy: 99.76\n",
      "Accuracy: 69.69\n",
      "Accuracy: 99.77\n",
      "Accuracy: 69.71\n",
      "Accuracy: 99.77\n",
      "Accuracy: 69.62\n",
      "Accuracy: 99.78\n",
      "Accuracy: 69.66\n",
      "Accuracy: 99.78\n",
      "Accuracy: 69.71\n",
      "Accuracy: 99.79\n",
      "Accuracy: 69.76\n",
      "Accuracy: 99.79\n",
      "Accuracy: 69.69\n",
      "Accuracy: 99.59\n",
      "Accuracy: 69.66\n",
      "Accuracy: 99.60\n",
      "Accuracy: 69.73\n",
      "Accuracy: 99.80\n",
      "Accuracy: 69.77\n",
      "Accuracy: 99.81\n",
      "Accuracy: 69.88\n",
      "Accuracy: 99.81\n",
      "Accuracy: 69.92\n",
      "Accuracy: 99.81\n",
      "Accuracy: 70.02\n",
      "Accuracy: 99.82\n",
      "Accuracy: 70.13\n"
     ]
    }
   ],
   "source": [
    "train_accuracies_al, test_accuracies_al,new_indice_entropy = active_learning(train_X, train_y, Pool_X, Pool_y, test_X, test_y, acquisition_function=entropy, num_iterations=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 37.25\n",
      "Accuracy: 18.81\n",
      "Accuracy: 51.92\n",
      "Accuracy: 31.70\n",
      "Accuracy: 71.70\n",
      "Accuracy: 39.22\n",
      "Accuracy: 85.19\n",
      "Accuracy: 44.86\n",
      "Accuracy: 87.27\n",
      "Accuracy: 48.73\n",
      "Accuracy: 94.64\n",
      "Accuracy: 51.17\n",
      "Accuracy: 98.25\n",
      "Accuracy: 51.56\n",
      "Accuracy: 100.00\n",
      "Accuracy: 51.84\n",
      "Accuracy: 100.00\n",
      "Accuracy: 52.83\n",
      "Accuracy: 98.33\n",
      "Accuracy: 54.59\n",
      "Accuracy: 96.72\n",
      "Accuracy: 56.86\n",
      "Accuracy: 93.55\n",
      "Accuracy: 58.42\n",
      "Accuracy: 92.06\n",
      "Accuracy: 59.33\n",
      "Accuracy: 93.75\n",
      "Accuracy: 60.12\n",
      "Accuracy: 95.38\n",
      "Accuracy: 60.95\n",
      "Accuracy: 95.45\n",
      "Accuracy: 61.42\n",
      "Accuracy: 95.52\n",
      "Accuracy: 61.91\n",
      "Accuracy: 95.59\n",
      "Accuracy: 62.16\n",
      "Accuracy: 97.10\n",
      "Accuracy: 62.30\n",
      "Accuracy: 97.14\n",
      "Accuracy: 62.38\n",
      "Accuracy: 97.18\n",
      "Accuracy: 62.22\n",
      "Accuracy: 95.83\n",
      "Accuracy: 61.91\n",
      "Accuracy: 95.89\n",
      "Accuracy: 61.69\n",
      "Accuracy: 95.95\n",
      "Accuracy: 61.38\n",
      "Accuracy: 94.67\n",
      "Accuracy: 61.33\n",
      "Accuracy: 94.74\n",
      "Accuracy: 61.39\n",
      "Accuracy: 94.81\n",
      "Accuracy: 61.57\n",
      "Accuracy: 93.59\n",
      "Accuracy: 61.75\n",
      "Accuracy: 93.67\n",
      "Accuracy: 61.61\n",
      "Accuracy: 96.25\n",
      "Accuracy: 61.73\n",
      "Accuracy: 96.30\n",
      "Accuracy: 61.67\n",
      "Accuracy: 95.12\n",
      "Accuracy: 61.80\n",
      "Accuracy: 96.39\n",
      "Accuracy: 62.12\n",
      "Accuracy: 95.24\n",
      "Accuracy: 62.41\n",
      "Accuracy: 96.47\n",
      "Accuracy: 62.75\n",
      "Accuracy: 96.51\n",
      "Accuracy: 63.22\n",
      "Accuracy: 96.55\n",
      "Accuracy: 63.57\n",
      "Accuracy: 97.73\n",
      "Accuracy: 64.05\n",
      "Accuracy: 97.75\n",
      "Accuracy: 64.52\n",
      "Accuracy: 97.78\n",
      "Accuracy: 65.09\n",
      "Accuracy: 97.80\n",
      "Accuracy: 65.88\n",
      "Accuracy: 97.83\n",
      "Accuracy: 66.14\n",
      "Accuracy: 97.85\n",
      "Accuracy: 66.59\n",
      "Accuracy: 96.81\n",
      "Accuracy: 67.03\n",
      "Accuracy: 96.84\n",
      "Accuracy: 67.33\n",
      "Accuracy: 97.92\n",
      "Accuracy: 67.48\n",
      "Accuracy: 97.94\n",
      "Accuracy: 67.75\n",
      "Accuracy: 97.96\n",
      "Accuracy: 67.94\n",
      "Accuracy: 98.99\n",
      "Accuracy: 67.98\n",
      "Accuracy: 99.00\n",
      "Accuracy: 68.19\n"
     ]
    }
   ],
   "source": [
    "train_accuracies, test_accuracies, new_indices_list_mc_dropout = active_learning_mc_dropout(train_X, train_y, Pool_X, Pool_y, test_X, test_y, num_iterations=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming your dataset is in NumPy arrays\n",
    "# train_X, train_y, test_X, test_y, Pool_X, Pool_y = ...\n",
    "\n",
    "# Reshape the data\n",
    "train_X = train_X.reshape(train_X.shape[0], -1)\n",
    "test_X = test_X.reshape(test_X.shape[0], -1)\n",
    "Pool_X = Pool_X.reshape(Pool_X.shape[0], -1)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "train_X_tensor = torch.tensor(train_X, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(train_y, dtype=torch.long)\n",
    "test_X_tensor = torch.tensor(test_X, dtype=torch.float32)\n",
    "test_y_tensor = torch.tensor(test_y, dtype=torch.long)\n",
    "Pool_X_tensor = torch.tensor(Pool_X, dtype=torch.float32)\n",
    "\n",
    "# Define batch size for DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoader for the unlabeled dataset (Pool_X_tensor)\n",
    "unlabeled_dataset = TensorDataset(Pool_X_tensor)\n",
    "unlabeled_data_loader = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define your SimpleNN model class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, enable_dropout=False):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if enable_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def enable_dropout(self):\n",
    "        \"\"\"\n",
    "        Enable dropout layers during model evaluation.\n",
    "        \"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.train()\n",
    "\n",
    "# Instantiate the SimpleNN model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_data_loader):\n",
    "        # Reshape images if necessary\n",
    "        images = images.view(-1, 28*28)\n",
    "\n",
    "        # Move images and labels to the appropriate device (e.g., GPU)\n",
    "        images = images.to(torch.device('cuda'))\n",
    "        labels = labels.to(torch.device('cuda'))\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training progress\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_data_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Active Learning with Epistemic Uncertainty using Monte Carlo Dropout\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_monte_carlo_predictions(data_loader, forward_passes, model, n_classes, n_samples):\n",
    "\n",
    "    dropout_predictions = np.empty((0, n_samples, n_classes))\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    for i in range(forward_passes):\n",
    "        predictions = np.empty((0, n_classes))\n",
    "        model.eval()\n",
    "        enable_dropout(model)\n",
    "        for i, image in enumerate(data_loader):\n",
    "            image = image.to(torch.device('cuda'))\n",
    "            with torch.no_grad():\n",
    "                output = model(image)\n",
    "                output = softmax(output)  \n",
    "            predictions = np.vstack((predictions, output.cpu().numpy()))\n",
    "\n",
    "        dropout_predictions = np.vstack((dropout_predictions, predictions[np.newaxis, :, :]))\n",
    "\n",
    "    mean = np.mean(dropout_predictions, axis=0)\n",
    "\n",
    "    return mean\n",
    "\n",
    "def enable_dropout(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.train()\n",
    "\n",
    "def entropy(p):\n",
    "    return -np.sum(p * np.log2(p + 1e-8), axis=1)  # Adding a small value to avoid log(0) errors\n",
    "\n",
    "def active_learning_with_uncertainty(data_loader, forward_passes, model, n_classes, n_samples, alpha=0.5, num_samples_to_label=100):\n",
    "    dropout_predictions = get_monte_carlo_predictions(data_loader, forward_passes, model, n_classes, n_samples)\n",
    "\n",
    "    # Calculate epistemic uncertainty using variance of predictions\n",
    "    variance = np.var(dropout_predictions, axis=0)\n",
    "\n",
    "    # Calculate entropy of the predicted probabilities\n",
    "    mean = np.mean(dropout_predictions, axis=0)\n",
    "    entropies = entropy(mean)\n",
    "\n",
    "    # Calculate the uncertainty score (a weighted combination of variance and entropy)\n",
    "    uncertainty_score = alpha * variance + (1 - alpha) * entropies\n",
    "\n",
    "    # Sort the samples based on the calculated uncertainty scores\n",
    "    sorted_indices = np.argsort(uncertainty_score)\n",
    "\n",
    "    # Select the most uncertain samples for labeling\n",
    "    selected_indices = sorted_indices[-num_samples_to_label:]\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "# Active Learning Loop\n",
    "num_iterations = 5\n",
    "num_samples_to_label = 100\n",
    "forward_passes = 10  # You can adjust the number of forward passes as per your requirements\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    print(f\"Active Learning Iteration {iteration + 1}/{num_iterations}\")\n",
    "    \n",
    "    # Step 1: Select the most uncertain samples from the unlabeled dataset\n",
    "    selected_indices = active_learning_with_uncertainty(\n",
    "        data_loader=unlabeled_data_loader,\n",
    "        forward_passes=forward_passes,\n",
    "        model=model,\n",
    "        n_classes=10,  # Replace with the actual number of classes in your dataset\n",
    "        n_samples=num_samples_to_label\n",
    "    )\n",
    "\n",
    "    # Step 2: Label the selected samples and add them to the training dataset\n",
    "    labeled_images = Pool_X_tensor[selected_indices]\n",
    "    labeled_labels = Pool_y_tensor[selected_indices]\n",
    "\n",
    "    # Combine the labeled samples with the original training dataset\n",
    "    train_X_tensor = torch.cat([train_X_tensor, labeled_images])\n",
    "    train_y_tensor = torch.cat([train_y_tensor, labeled_labels])\n",
    "\n",
    "    # Create a new DataLoader for the updated training dataset\n",
    "    train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Step 3: Retrain the model on the updated training dataset\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, (images, labels) in enumerate(train_data_loader):\n",
    "            # Reshape images if necessary\n",
    "            images = images.view(-1, 28*28)\n",
    "\n",
    "            # Move images and labels to the appropriate device (e.g., GPU)\n",
    "            images = images.to(torch.device('cuda'))\n",
    "            labels = labels.to(torch.device('cuda'))\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print training progress\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_data_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# After the active learning iterations, your model should be trained on the updated dataset.\n",
    "# You can now evaluate the model on the test dataset to measure its performance.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_X_tensor = test_X_tensor.to(torch.device('cuda'))\n",
    "    test_y_tensor = test_y_tensor.to(torch.device('cuda'))\n",
    "    outputs = model(test_X_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == test_y_tensor).sum().item() / len(test_y_tensor)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
