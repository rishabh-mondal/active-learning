{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 02:43:47.784616: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-02 02:43:47.833307: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-02 02:43:48.942045: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib.gridspec import GridSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (9600, 28, 28, 1)\n",
      "Train labels shape: (9600,)\n",
      "Test data shape: (38400, 28, 28, 1)\n",
      "Test labels shape: (38400,)\n",
      "Test data shape: (12000, 28, 28, 1)\n",
      "Test labels shape: (12000,)\n"
     ]
    }
   ],
   "source": [
    "def split_dataset_with_equal_class_representation(train_data, train_labels, test_ratio=0.2):\n",
    "    classes = np.unique(train_labels)\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    # Split data into class-specific subsets\n",
    "    class_data = [train_data[train_labels == c] for c in classes]\n",
    "\n",
    "    # Calculate the number of samples to include from each class in the test set\n",
    "    num_samples_per_class = int(len(train_data) * test_ratio / num_classes)\n",
    "\n",
    "    # Initialize train and test datasets\n",
    "    train_data_split = []\n",
    "    test_data_split = []\n",
    "    train_labels_split = []\n",
    "    test_labels_split = []\n",
    "\n",
    "    # Split data for each class\n",
    "    for c in range(num_classes):\n",
    "        data_c = class_data[c]\n",
    "        num_samples_test_c = min(num_samples_per_class, len(data_c))\n",
    "\n",
    "        # Randomly shuffle the data\n",
    "        np.random.shuffle(data_c)\n",
    "\n",
    "        # Split data for class c into train and test sets\n",
    "        train_data_c = data_c[:-num_samples_test_c]\n",
    "        test_data_c = data_c[-num_samples_test_c:]\n",
    "\n",
    "        # Assign labels for each split\n",
    "        train_labels_c = np.full(len(train_data_c), classes[c])\n",
    "        test_labels_c = np.full(len(test_data_c), classes[c])\n",
    "\n",
    "        # Append class-specific data to overall train and test datasets\n",
    "        train_data_split.append(train_data_c)\n",
    "        test_data_split.append(test_data_c)\n",
    "        train_labels_split.append(train_labels_c)\n",
    "        test_labels_split.append(test_labels_c)\n",
    "\n",
    "    # Concatenate class-specific data to create final train and test datasets\n",
    "    train_data_final = np.concatenate(train_data_split, axis=0)\n",
    "    test_data_final = np.concatenate(test_data_split, axis=0)\n",
    "    train_labels_final = np.concatenate(train_labels_split, axis=0)\n",
    "    test_labels_final = np.concatenate(test_labels_split, axis=0)\n",
    "\n",
    "    # Shuffle the data again to ensure randomness\n",
    "    train_indices = np.arange(len(train_data_final))\n",
    "    np.random.shuffle(train_indices)\n",
    "    train_data_final = train_data_final[train_indices]\n",
    "    train_labels_final = train_labels_final[train_indices]\n",
    "\n",
    "    test_indices = np.arange(len(test_data_final))\n",
    "    np.random.shuffle(test_indices)\n",
    "    test_data_final = test_data_final[test_indices]\n",
    "    test_labels_final = test_labels_final[test_indices]\n",
    "\n",
    "    return (train_data_final, train_labels_final), (test_data_final, test_labels_final)\n",
    "\n",
    "# Load MNIST dataset\n",
    "(train_data, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to [0, 1]\n",
    "train_data = train_data.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data to 4D tensor (number of samples, height, width, channels)\n",
    "train_data = np.expand_dims(train_data, axis=-1)\n",
    "\n",
    "# Split data into train and test sets with equal class representation\n",
    "(train_data, train_labels), (test_X, test_y) = split_dataset_with_equal_class_representation(train_data, train_labels, test_ratio=0.2)\n",
    "(train_X, train_y), (Pool_X, Pool_y) = split_dataset_with_equal_class_representation(train_data, train_labels, test_ratio=0.8)\n",
    "\n",
    "print(\"Train data shape:\", train_X.shape)\n",
    "print(\"Train labels shape:\", train_y.shape)\n",
    "print(\"Test data shape:\", Pool_X.shape)\n",
    "print(\"Test labels shape:\", Pool_y.shape)\n",
    "print(\"Test data shape:\", test_X.shape)\n",
    "print(\"Test labels shape:\", test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9600, 784) (12000, 784) (38400, 784)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_X = train_X.reshape(train_X.shape[0], -1)\n",
    "test_X = test_X.reshape(test_X.shape[0], -1)\n",
    "Pool_X = Pool_X.reshape(Pool_X.shape[0], -1)\n",
    "print(train_X.shape, test_X.shape, Pool_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/150], Loss: 0.3839\n",
      "Epoch [2/10], Step [100/150], Loss: 0.2842\n",
      "Epoch [3/10], Step [100/150], Loss: 0.2251\n",
      "Epoch [4/10], Step [100/150], Loss: 0.1637\n",
      "Epoch [5/10], Step [100/150], Loss: 0.1109\n",
      "Epoch [6/10], Step [100/150], Loss: 0.1223\n",
      "Epoch [7/10], Step [100/150], Loss: 0.0940\n",
      "Epoch [8/10], Step [100/150], Loss: 0.1545\n",
      "Epoch [9/10], Step [100/150], Loss: 0.1319\n",
      "Epoch [10/10], Step [100/150], Loss: 0.2385\n"
     ]
    }
   ],
   "source": [
    "train_X_tensor = torch.tensor(train_X, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(train_y, dtype=torch.long)\n",
    "test_X_tensor = torch.tensor(test_X, dtype=torch.float32)\n",
    "test_y_tensor = torch.tensor(test_y, dtype=torch.long)\n",
    "Pool_X_tensor = torch.tensor(Pool_X, dtype=torch.float32)\n",
    "Pool_y_tensor = torch.tensor(Pool_y, dtype=torch.long)\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoader for the unlabeled dataset (Pool_X_tensor)\n",
    "unlabeled_dataset = TensorDataset(Pool_X_tensor)\n",
    "unlabeled_data_loader = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define your SimpleNN model class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, enable_dropout=False):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if enable_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def enable_dropout(self):\n",
    "        \"\"\"\n",
    "        Enable dropout layers during model evaluation.\n",
    "        \"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.train()\n",
    "\n",
    "# Instantiate the SimpleNN model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_data_loader):\n",
    "        # Reshape images if necessary\n",
    "        images = images.view(-1, 28*28)\n",
    "\n",
    "        # Move images and labels to the appropriate device (e.g., GPU)\n",
    "        images = images\n",
    "        labels = labels\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training progress\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_data_loader)}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def enable_dropout(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_monte_carlo_predictions(data_loader, forward_passes, model, n_classes, n_samples):\n",
    "    dropout_predictions = []\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    for i in range(forward_passes):\n",
    "        predictions = []\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        enable_dropout(model)\n",
    "\n",
    "        for batch_images in data_loader:\n",
    "            for image in batch_images:\n",
    "                # Move each image to the appropriate device (GPU or CPU)\n",
    "                image = image\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = model(image)\n",
    "                    output = softmax(output)\n",
    "\n",
    "                predictions.append(output.cpu().numpy())\n",
    "\n",
    "        dropout_predictions.append(np.array(predictions))\n",
    "\n",
    "    return np.array(dropout_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    return -np.sum(p * np.log2(p + 1e-8), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def active_learning_with_uncertainty(data_loader, forward_passes, model, n_classes, n_samples, alpha=0.5, num_samples_to_label=100):\n",
    "    dropout_predictions = get_monte_carlo_predictions(data_loader, forward_passes, model, n_classes, n_samples)\n",
    "    mean = np.mean(dropout_predictions, axis=0)\n",
    "    entropies = entropy(mean)\n",
    "    uncertainty_score = entropies\n",
    "    sorted_indices = np.argsort(uncertainty_score)\n",
    "    selected_indices = sorted_indices[-num_samples_to_label:]\n",
    "\n",
    "    return selected_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active Learning Iteration 1/5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 1 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m# Combine the labeled samples with the original training dataset\u001b[39;00m\n\u001b[1;32m     28\u001b[0m train_X_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([train_X_tensor, labeled_images])\n\u001b[0;32m---> 29\u001b[0m train_y_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([train_y_tensor, labeled_labels])  \u001b[39m# Concatenate the labels directly\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# Create a new DataLoader for the updated training dataset\u001b[39;00m\n\u001b[1;32m     32\u001b[0m train_dataset \u001b[39m=\u001b[39m TensorDataset(train_X_tensor, train_y_tensor)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 1 and 2"
     ]
    }
   ],
   "source": [
    "\n",
    "num_iterations = 5\n",
    "num_samples_to_label = 100\n",
    "forward_passes = 10  \n",
    "\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    print(f\"Active Learning Iteration {iteration + 1}/{num_iterations}\")\n",
    "\n",
    "    # Step 1: Select the most uncertain samples from the unlabeled dataset\n",
    "    selected_indices = active_learning_with_uncertainty(\n",
    "        data_loader=unlabeled_data_loader,\n",
    "        forward_passes=forward_passes,\n",
    "        model=model,\n",
    "        n_classes=10,  # Replace with the actual number of classes in your dataset\n",
    "        n_samples=num_samples_to_label\n",
    "    )\n",
    "\n",
    "    # Step 2: Label the selected samples and add them to the training dataset\n",
    "    labeled_images = Pool_X_tensor[selected_indices]\n",
    "    labeled_labels = Pool_y_tensor[selected_indices]\n",
    "\n",
    "    # Reshape labeled_images to remove the extra dimension\n",
    "    labeled_images = labeled_images.view(-1, labeled_images.size(-1))\n",
    "\n",
    "    # Combine the labeled samples with the original training dataset\n",
    "    train_X_tensor = torch.cat([train_X_tensor, labeled_images])\n",
    "    train_y_tensor = torch.cat([train_y_tensor, labeled_labels])  # Concatenate the labels directly\n",
    "\n",
    "    # Create a new DataLoader for the updated training dataset\n",
    "    train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Step 3: Retrain the model on the updated training dataset\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, (images, labels) in enumerate(train_data_loader):\n",
    "            # Move images and labels to the appropriate device (GPU or CPU)\n",
    "            images = images\n",
    "            labels = labels\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print training progress\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_data_loader)}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After the active learning iterations, your model should be trained on the updated dataset.\n",
    "# You can now evaluate the model on the test dataset to measure its performance.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_X_tensor = test_X_tensor.to(device)  # Move the test data to the same device as the model\n",
    "    test_y_tensor = test_y_tensor.to(device)  # Move the test data to the same device as the model\n",
    "    outputs = model(test_X_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == test_y_tensor).sum().item() / len(test_y_tensor)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
